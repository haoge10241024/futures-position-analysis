#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœŸè´§æŒä»“åˆ†æç³»ç»Ÿ - æ€§èƒ½ä¼˜åŒ–æ¨¡å—
ä¸“é—¨ç”¨äºæå‡äº‘ç«¯éƒ¨ç½²æ€§èƒ½
ä½œè€…ï¼š7haoge
é‚®ç®±ï¼š953534947@qq.com
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import pickle
import hashlib
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import concurrent.futures
from functools import wraps
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = cache_dir
        self.ensure_cache_directory()
        self.session = self.create_optimized_session()
        
    def ensure_cache_directory(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def create_optimized_session(self):
        """åˆ›å»ºä¼˜åŒ–çš„HTTPä¼šè¯"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # è®¾ç½®è¶…æ—¶
        session.timeout = 30
        
        return session
    
    def get_cache_key(self, func_name: str, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{func_name}_{str(args)}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get_cache_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def is_cache_valid(self, cache_path: str, max_age_hours: int = 24) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_path):
            return False
        
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        return datetime.now() - file_time < timedelta(hours=max_age_hours)
    
    def save_to_cache(self, cache_key: str, data: Any):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            cache_path = self.get_cache_path(cache_key)
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            st.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    def load_from_cache(self, cache_key: str) -> Optional[Any]:
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        try:
            cache_path = self.get_cache_path(cache_key)
            if self.is_cache_valid(cache_path):
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
        except Exception as e:
            st.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
        return None
    
    def clear_old_cache(self, max_age_days: int = 7):
        """æ¸…ç†æ—§ç¼“å­˜"""
        try:
            cutoff_time = datetime.now() - timedelta(days=max_age_days)
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    file_path = os.path.join(self.cache_dir, filename)
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if file_time < cutoff_time:
                        os.remove(file_path)
        except Exception as e:
            st.warning(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")

# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
optimizer = PerformanceOptimizer()

def smart_cache(max_age_hours: int = 24):
    """æ™ºèƒ½ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = optimizer.get_cache_key(func.__name__, *args, **kwargs)
            
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_data = optimizer.load_from_cache(cache_key)
            if cached_data is not None:
                st.info(f"âœ… ä½¿ç”¨ç¼“å­˜æ•°æ® - {func.__name__}")
                return cached_data
            
            # æ‰§è¡Œå‡½æ•°
            st.info(f"ğŸ”„ æ­£åœ¨è·å–æ–°æ•°æ® - {func.__name__}")
            result = func(*args, **kwargs)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if result is not None:
                optimizer.save_to_cache(cache_key, result)
            
            return result
        return wrapper
    return decorator

@st.cache_data(ttl=3600)  # Streamlitç¼“å­˜1å°æ—¶
def cached_data_fetch(func_name: str, date: str, exchange: str = None):
    """ç¼“å­˜çš„æ•°æ®è·å–å‡½æ•° - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«è¶…æ—¶æ§åˆ¶"""
    import akshare as ak
    import signal
    import time
    
    def timeout_handler(signum, frame):
        raise TimeoutError("æ•°æ®è·å–è¶…æ—¶")
    
    try:
        # ä¸ºå¹¿æœŸæ‰€è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        if func_name == "futures_gfex_position_rank":
            timeout_seconds = 30  # å¹¿æœŸæ‰€30ç§’è¶…æ—¶ï¼ˆä»15ç§’å¢åŠ ï¼‰
        else:
            timeout_seconds = 30  # å…¶ä»–äº¤æ˜“æ‰€30ç§’è¶…æ—¶
        
        # è®¾ç½®è¶…æ—¶ä¿¡å·ï¼ˆä»…åœ¨éWindowsç³»ç»Ÿï¼‰
        if hasattr(signal, 'SIGALRM'):
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout_seconds)
        
        start_time = time.time()
        
        if func_name == "futures_dce_position_rank":
            result = ak.futures_dce_position_rank(date=date)
        elif func_name == "get_cffex_rank_table":
            result = ak.get_cffex_rank_table(date=date)
        elif func_name == "get_czce_rank_table":
            result = ak.get_czce_rank_table(date=date)
        elif func_name == "get_shfe_rank_table":
            result = ak.get_shfe_rank_table(date=date)
        elif func_name == "futures_gfex_position_rank":
            # å¹¿æœŸæ‰€ç‰¹æ®Šå¤„ç† - æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 3  # å¢åŠ é‡è¯•æ¬¡æ•°
            for attempt in range(max_retries):
                try:
                    if attempt > 0:
                        st.info(f"ğŸ”„ å¹¿æœŸæ‰€æ•°æ®è·å–é‡è¯• {attempt}/{max_retries-1}")
                    result = ak.futures_gfex_position_rank(date=date)
                    break
                except Exception as e:
                    if attempt == max_retries - 1:
                        st.warning(f"å¹¿æœŸæ‰€æ•°æ®è·å–å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰ï¼Œè·³è¿‡è¯¥äº¤æ˜“æ‰€: {str(e)}")
                        return None
                    time.sleep(3)  # é‡è¯•å‰ç­‰å¾…3ç§’ï¼ˆå¢åŠ ç­‰å¾…æ—¶é—´ï¼‰
        elif func_name == "get_futures_daily" and exchange:
            result = ak.get_futures_daily(start_date=date, end_date=date, market=exchange)
        else:
            return None
        
        # å–æ¶ˆè¶…æ—¶ä¿¡å·
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)
        
        elapsed_time = time.time() - start_time
        if elapsed_time > 8:  # å¦‚æœè¶…è¿‡8ç§’ï¼Œæ˜¾ç¤ºæç¤º
            st.info(f"â±ï¸ {func_name} æ•°æ®è·å–è€—æ—¶ {elapsed_time:.1f} ç§’")
        
        return result
        
    except TimeoutError:
        st.warning(f"â° {func_name} æ•°æ®è·å–è¶…æ—¶ï¼Œè·³è¿‡è¯¥äº¤æ˜“æ‰€")
        return None
    except Exception as e:
        st.warning(f"âŒ {func_name} æ•°æ®è·å–å¤±è´¥: {str(e)}")
        return None
    finally:
        # ç¡®ä¿æ¸…ç†è¶…æ—¶ä¿¡å·
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)

class FastDataManager:
    """å¿«é€Ÿæ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.optimizer = optimizer
        
        # äº¤æ˜“æ‰€é…ç½® - æŒ‰ä¼˜å…ˆçº§æ’åº
        self.exchange_config = {
            "å¤§å•†æ‰€": {
                "func_name": "futures_dce_position_rank",
                "filename": "å¤§å•†æ‰€æŒä»“.xlsx",
                "priority": 1,
                "timeout": 30
            },
            "ä¸­é‡‘æ‰€": {
                "func_name": "get_cffex_rank_table", 
                "filename": "ä¸­é‡‘æ‰€æŒä»“.xlsx",
                "priority": 2,
                "timeout": 30
            },
            "éƒ‘å•†æ‰€": {
                "func_name": "get_czce_rank_table",
                "filename": "éƒ‘å•†æ‰€æŒä»“.xlsx", 
                "priority": 3,
                "timeout": 30
            },
            "ä¸ŠæœŸæ‰€": {
                "func_name": "get_shfe_rank_table",
                "filename": "ä¸ŠæœŸæ‰€æŒä»“.xlsx",
                "priority": 4,
                "timeout": 30
            },
            "å¹¿æœŸæ‰€": {
                "func_name": "futures_gfex_position_rank",
                "filename": "å¹¿æœŸæ‰€æŒä»“.xlsx",
                "priority": 5,
                "timeout": 30
            }
        }
    
    def fetch_position_data_fast(self, trade_date: str, progress_callback=None) -> bool:
        """å¿«é€Ÿè·å–æŒä»“æ•°æ® - ä½¿ç”¨å¹¶å‘å’Œç¼“å­˜ï¼Œä¼˜åŒ–å¹¿æœŸæ‰€å¤„ç†"""
        success_count = 0
        total_exchanges = len(self.exchange_config)
        failed_exchanges = []
        
        # é‡æ–°æ’åºï¼Œå°†å¹¿æœŸæ‰€æ”¾åˆ°æœ€åå¤„ç†
        sorted_exchanges = sorted(
            self.exchange_config.items(), 
            key=lambda x: x[1]['priority']
        )
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è·å–æ•°æ®ï¼Œä½†é™åˆ¶å¹¶å‘æ•°é¿å…ç½‘ç»œæ‹¥å µ
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_to_exchange = {}
            
            for exchange_name, config in sorted_exchanges:
                future = executor.submit(
                    self._fetch_single_exchange_data_with_timeout,
                    exchange_name, config, trade_date
                )
                future_to_exchange[future] = exchange_name
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for i, future in enumerate(concurrent.futures.as_completed(future_to_exchange)):
                exchange_name = future_to_exchange[future]
                
                if progress_callback:
                    progress = (i + 1) / total_exchanges * 0.6
                    progress_callback(f"å·²å®Œæˆ {exchange_name} æ•°æ®è·å–", progress)
                
                try:
                    success = future.result(timeout=60)  # 60ç§’æ€»è¶…æ—¶ï¼ˆå¢åŠ è¶…æ—¶æ—¶é—´ï¼‰
                    if success:
                        success_count += 1
                        st.success(f"âœ… {exchange_name} æ•°æ®è·å–æˆåŠŸ")
                    else:
                        failed_exchanges.append(exchange_name)
                        st.warning(f"âš ï¸ {exchange_name} æ•°æ®è·å–å¤±è´¥ï¼Œä½†ä¸å½±å“å…¶ä»–äº¤æ˜“æ‰€")
                except Exception as e:
                    failed_exchanges.append(exchange_name)
                    st.warning(f"âš ï¸ {exchange_name} æ•°æ®è·å–å¼‚å¸¸: {str(e)}")
                    continue
        
        if progress_callback:
            progress_callback("æŒä»“æ•°æ®è·å–å®Œæˆ", 0.6)
        
        # æ˜¾ç¤ºè·å–ç»“æœæ‘˜è¦
        if success_count > 0:
            st.info(f"ğŸ“Š æˆåŠŸè·å– {success_count}/{total_exchanges} ä¸ªäº¤æ˜“æ‰€æ•°æ®")
            if failed_exchanges:
                st.info(f"âš ï¸ æœªè·å–åˆ°æ•°æ®çš„äº¤æ˜“æ‰€: {', '.join(failed_exchanges)}")
        
        return success_count > 0
    
    def _fetch_single_exchange_data_with_timeout(self, exchange_name: str, config: dict, trade_date: str) -> bool:
        """è·å–å•ä¸ªäº¤æ˜“æ‰€æ•°æ® - å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«ç‰¹æ®Šå¤„ç†"""
        try:
            # å¹¿æœŸæ‰€ç‰¹æ®Šå¤„ç†
            if exchange_name == "å¹¿æœŸæ‰€":
                st.info(f"ğŸ”„ æ­£åœ¨å°è¯•è·å–{exchange_name}æ•°æ®ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰...")
            
            # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®è·å–å‡½æ•°
            data_dict = cached_data_fetch(config["func_name"], trade_date)
            
            if data_dict:
                # ä¿å­˜åˆ°Excel
                save_path = os.path.join(self.data_dir, config['filename'])
                with pd.ExcelWriter(save_path, engine='openpyxl') as writer:
                    for sheet_name, df in data_dict.items():
                        # æ¸…ç†sheetåç§°
                        clean_name = sheet_name[:31].replace("/", "-").replace("*", "")
                        df.to_excel(writer, sheet_name=clean_name, index=False)
                
                return True
            else:
                # å¦‚æœæ˜¯å¹¿æœŸæ‰€å¤±è´¥ï¼Œç»™å‡ºç‰¹æ®Šæç¤º
                if exchange_name == "å¹¿æœŸæ‰€":
                    st.info("â„¹ï¸ å¹¿æœŸæ‰€æ•°æ®æš‚æ—¶æ— æ³•è·å–ï¼Œè¿™æ˜¯å¸¸è§æƒ…å†µï¼Œä¸å½±å“å…¶ä»–åˆ†æ")
                return False
            
        except Exception as e:
            if exchange_name == "å¹¿æœŸæ‰€":
                st.info(f"â„¹ï¸ å¹¿æœŸæ‰€æ•°æ®è·å–é‡åˆ°é—®é¢˜: {str(e)}ï¼Œç»§ç»­å¤„ç†å…¶ä»–äº¤æ˜“æ‰€")
            else:
                st.warning(f"è·å–{exchange_name}æ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    @smart_cache(max_age_hours=6)
    def fetch_price_data_fast(self, trade_date: str, progress_callback=None) -> pd.DataFrame:
        """å¿«é€Ÿè·å–æœŸè´§è¡Œæƒ…æ•°æ®"""
        price_exchanges = [
            {"market": "DCE", "name": "å¤§å•†æ‰€"},
            {"market": "CFFEX", "name": "ä¸­é‡‘æ‰€"}, 
            {"market": "CZCE", "name": "éƒ‘å•†æ‰€"},
            {"market": "SHFE", "name": "ä¸ŠæœŸæ‰€"},
        ]
        
        all_data = []
        
        # å¹¶å‘è·å–è¡Œæƒ…æ•°æ®
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_to_exchange = {}
            
            for exchange in price_exchanges:
                future = executor.submit(
                    cached_data_fetch,
                    "get_futures_daily",
                    trade_date,
                    exchange["market"]
                )
                future_to_exchange[future] = exchange
            
            for i, future in enumerate(concurrent.futures.as_completed(future_to_exchange)):
                exchange = future_to_exchange[future]
                
                if progress_callback:
                    progress = 0.6 + (i / len(price_exchanges)) * 0.2
                    progress_callback(f"å·²å®Œæˆ {exchange['name']} è¡Œæƒ…æ•°æ®", progress)
                
                try:
                    df = future.result(timeout=30)
                    if df is not None and not df.empty:
                        df['exchange'] = exchange["name"]
                        all_data.append(df)
                except Exception as e:
                    st.warning(f"è·å–{exchange['name']}è¡Œæƒ…æ•°æ®å¤±è´¥: {str(e)}")
                    continue
        
        if progress_callback:
            progress_callback("è¡Œæƒ…æ•°æ®è·å–å®Œæˆ", 0.8)
        
        return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()

def optimize_streamlit_performance():
    """ä¼˜åŒ–Streamlitæ€§èƒ½"""
    # æ¸…ç†æ—§ç¼“å­˜
    optimizer.clear_old_cache()
    
    # è®¾ç½®Streamlité…ç½®
    if 'performance_optimized' not in st.session_state:
        st.session_state.performance_optimized = True
        
        # æ˜¾ç¤ºæ€§èƒ½æç¤º
        st.info("""
        ğŸš€ **æ€§èƒ½ä¼˜åŒ–å·²å¯ç”¨**
        - âœ… æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
        - âœ… å¹¶å‘æ•°æ®è·å–  
        - âœ… ç½‘ç»œè¿æ¥ä¼˜åŒ–
        - âœ… è‡ªåŠ¨ç¼“å­˜æ¸…ç†
        
        é¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼Œåç»­ä¼šæ˜¾è‘—åŠ é€Ÿï¼
        """)

def show_performance_metrics():
    """æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡"""
    cache_files = [f for f in os.listdir(optimizer.cache_dir) if f.endswith('.pkl')]
    cache_size = len(cache_files)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ç¼“å­˜æ–‡ä»¶æ•°", cache_size)
    
    with col2:
        if cache_size > 0:
            latest_cache = max(
                [os.path.join(optimizer.cache_dir, f) for f in cache_files],
                key=os.path.getmtime
            )
            cache_time = datetime.fromtimestamp(os.path.getmtime(latest_cache))
            st.metric("æœ€æ–°ç¼“å­˜", cache_time.strftime("%H:%M"))
        else:
            st.metric("æœ€æ–°ç¼“å­˜", "æ— ")
    
    with col3:
        if st.button("ğŸ—‘ï¸ æ¸…ç†ç¼“å­˜"):
            optimizer.clear_old_cache(max_age_days=0)  # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            st.success("ç¼“å­˜å·²æ¸…ç†")
            st.rerun() 
